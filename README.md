<div align="center">

<h1>Scaling Language-centric Omnimodal Representation Learning</h1>

<div>
    <a target='_blank'>Chenghao Xiao,</a>&emsp;
    <a target='_blank'>Hou Pong Chan<sup>â€ </sup>,</a>&emsp;
    <a target='_blank'>Hao Zhang<sup>â€ </sup>,</a>&emsp;
    <a target='_blank'>Weiwen Xu,</a>&emsp;
    <a target='_blank'>Mahani Aljunied,</a>&emsp;
    <a target='_blank'>Yu Rong<sup>â€¡</sup></a>&emsp;
</div>

<div>
    <em>DAMO Academy, Alibaba Group</em>&emsp;
</div>
<em><sup>â€ </sup>Corresponding Authors &emsp;<sup>â€¡</sup>Project Head</em>

<h5 align="center">

[![arXiv](https://img.shields.io/badge/Arxiv-2510.11693-AD1C18.svg?logo=arXiv)](https://arxiv.org/abs/2510.11693)
[![hf_paper](https://img.shields.io/badge/ğŸ¤—-Paper%20In%20HF-red.svg)](https://huggingface.co/papers/2510.11693)
[![hf_collection](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-Collections-blue.svg)](https://huggingface.co/MMR1/datasets)
<br>
</h5> 


</div>

<h5 align="center"> ğŸŒŸ This repo contains the codes and datasets for the paper "Scaling Language-centric Omnimodal Representation Learning" to appear at NeurIPS 2025. If our project helps you, please give us a star â­ on GitHub and upvote our HF paper to support us. ğŸ™ğŸ™ </h2>


<h2>ğŸ‰ Updates</h2>

- **[2025-10]** Check out our [paper](https://huggingface.co/papers/2510.11693) on Huggingface Daily Papers.
- **[2025-09]** Our paper is accepted by NeurIPS 2025.

<h2>Overview</h2>

- We introduce **LCO-Embedding**, a language-centric omnimodal representation learning method and the LCO-Embedding model families, setting a new state-of-the-art on [MIEB](https://huggingface.co/blog/isaacchung/introducing-mieb) (Massive Image Embedding Benchmark), while supporting audio and videos.
- We introduce the **Generation-Representation Scaling Law**, and connect models' generative capabilities and their representation upper bound.
- We introduce **SeaDoc**, a challenging visual document retrieval task in Southeast Asian languages, and show that continual generative pretraining before contrastive learning raises the representation upper bound.

<div align='center'><img src="https://cdn-uploads.huggingface.co/production/uploads/604f67ef0fe8ff3ec13d71ef/4Wd8fDFBdT6GxqN6-KzZN.png" alt="overview" width="80%"/></div>

We are updating all code and resources.
